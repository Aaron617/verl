# config for actor, rollout and reference model
actor_rollout_ref:

  # Whether it's a hybrid engine, currently only supports hybrid engine
  hybrid_engine: true

  # common configs for the model
  model:

    # Huggingface model path. This can be either local path or HDFS path.
    path: ~/models/deepseek-llm-7b-chat

    # Whether to use shared memory (SHM) for accelerating the loading of model weights
    use_shm: false

    # Additional Python packages to register huggingface models/tokenizers.
    external_lib: null

    # Used to override model's original configurations, mainly dropout
    override_config: {}

    # Enable gradient checkpointing for actor
    enable_gradient_checkpointing: true

    # Enable activation offloading for actor
    enable_activation_offload: false

    # Whether to remove padding tokens in inputs during training
    use_remove_padding: false

    # Set to positive value to enable LoRA (e.g., 32)
    lora_rank: 0

    # LoRA scaling factor
    lora_alpha: 16

    # Target modules to apply LoRA. Options: "all-linear" or
    # [q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj]
    target_modules: all-linear

    # Whether to use Liger for linear layer fusion
    use_liger: false

    # Whether to use custom fused kernels (e.g., FlashAttention, fused MLP)
    use_fused_kernels: false

    # Options for fused kernels. If use_fused_kernels is true, this will be used.
    fused_kernel_options:

      # Implementation backend for fused kernels. Options: "triton" or "torch".
      impl_backend: torch

    # Whether to enable loading a remote code model
    trust_remote_code: false

  # actor configs
  actor:

    # fsdp, fsdp2 or megatron. fsdp backend used here.
    strategy: fsdp

    # Split each sample into sub-batches of this size for PPO
    ppo_mini_batch_size: 256

    # [Deprecated] Global micro batch size
    ppo_micro_batch_size: null

    # Local per-GPU micro batch size
    ppo_micro_batch_size_per_gpu: null

    # Whether to automatically adjust batch size at runtime
    use_dynamic_bsz: false

    # Max tokens per GPU in one PPO batch; affects gradient accumulation
    # Typically it should be: n * ${data.max_prompt_length} + ${data.max_response_length}
    ppo_max_token_len_per_gpu: 16384

    # Gradient clipping for actor updates
    grad_clip: 1.0

    # PPO clip ratio
    clip_ratio: 0.2

    # Lower bound for asymmetric clipping (used in dual-clip PPO)
    clip_ratio_low: 0.2

    # Upper bound for asymmetric clipping (used in dual-clip PPO)
    clip_ratio_high: 0.2

    # policy loss config
    policy_loss:
      # Loss function mode: vanilla / clip-cov / kl-cov from https://arxiv.org/abs/2505.
      loss_mode: "vanilla"
      
      # Ratio of tokens to be clipped for clip-cov loss
      clip_cov_ratio: 0.0002

      # Lower bound for clip-cov loss
      clip_cov_lb: 1.0

      # Upper bound for clip-cov loss
      clip_cov_ub: 5.0

      # Ratio of tokens to be applied kl penalty for kl-cov loss
      kl_cov_ratio: 0.0002

      # KL divergence penalty coefficient
      ppo_kl_coef: 0.1
